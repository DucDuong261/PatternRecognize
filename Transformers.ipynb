{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2f10e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# raw data \n",
    "raw_data = [\n",
    "    (\"hello\", \"xin chào\"),\n",
    "    (\"how are you\", \"bạn khỏe không\"),\n",
    "    (\"i am fine\", \"mình khỏe\"),\n",
    "    (\"thank you\", \"cảm ơn\"),\n",
    "    (\"what is your name\", \"tên bạn là gì\"),\n",
    "    (\"my name is john\", \"tên mình là john\"),\n",
    "    (\"good morning\", \"chào buổi sáng\"),\n",
    "    (\"good night\", \"chúc ngủ ngon\"),\n",
    "    (\"see you later\", \"hẹn gặp lại\"),\n",
    "    (\"i love you\", \"tôi yêu bạn\"),\n",
    "]\n",
    "raw_data = [(s.lower().strip(), t.lower().strip()) for s, t in raw_data]\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# Building vocab \n",
    "def build_vocab(pairs, min_freq=1):\n",
    "    freq = {}\n",
    "    for s, t in pairs:\n",
    "        for w in (s.split() + t.split()):\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    itos = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted([w for w, c in freq.items() if c >= min_freq])\n",
    "    stoi = {w: i for i, w in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "src_stoi, src_itos = build_vocab(raw_data)\n",
    "tgt_stoi, tgt_itos = src_stoi, src_itos\n",
    "SRC_VOCAB_SIZE = len(src_itos)\n",
    "TGT_VOCAB_SIZE = len(tgt_itos)\n",
    "print(\"Vocab size:\", SRC_VOCAB_SIZE)\n",
    "\n",
    "# Encode sentence\n",
    "MAX_LEN = 10\n",
    "def encode_sentence(sent, stoi, max_len=MAX_LEN):\n",
    "    tokens = sent.split()\n",
    "    ids = [stoi.get(w, stoi[UNK_TOKEN]) for w in tokens]\n",
    "    ids = [stoi[BOS_TOKEN]] + ids + [stoi[EOS_TOKEN]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[PAD_TOKEN]] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len-1] + [stoi[EOS_TOKEN]]\n",
    "    return ids\n",
    "\n",
    "# Dataset \n",
    "class TinyTranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_stoi, tgt_stoi, max_len=MAX_LEN):\n",
    "        self.pairs = pairs\n",
    "        self.src_stoi = src_stoi\n",
    "        self.tgt_stoi = tgt_stoi\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s, t = self.pairs[idx]\n",
    "        src_ids = torch.tensor(encode_sentence(s, self.src_stoi, self.max_len), dtype=torch.long)\n",
    "        tgt_ids = torch.tensor(encode_sentence(t, self.tgt_stoi, self.max_len), dtype=torch.long)\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "dataset = TinyTranslationDataset(raw_data, src_stoi, tgt_stoi, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "#POS ENCODING\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Model \n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, nhead=4,\n",
    "                 num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=128,\n",
    "                 dropout=0.1, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.pos_encoder(self.src_tok_emb(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoder(self.tgt_tok_emb(tgt) * math.sqrt(self.d_model))\n",
    "        src_pad_mask = (src == src_stoi[PAD_TOKEN])\n",
    "        tgt_pad_mask = (tgt == tgt_stoi[PAD_TOKEN])\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_pad_mask,\n",
    "                               tgt_key_padding_mask=tgt_pad_mask,\n",
    "                               memory_key_padding_mask=src_pad_mask)\n",
    "        return self.output_layer(out)\n",
    "\n",
    "# Train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerSeq2Seq(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[PAD_TOKEN])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def shift_right(tgt_batch): return tgt_batch[:, :-1]\n",
    "def target_for_loss(tgt_batch): return tgt_batch[:, 1:]\n",
    "\n",
    "EPOCHS = 120\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for src_batch, tgt_batch in dataloader:\n",
    "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "        decoder_input = shift_right(tgt_batch)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src_batch, decoder_input)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)),\n",
    "                         target_for_loss(tgt_batch).contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    if epoch % 30 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} - loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Loss figure\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(losses, label=\"Training Loss\", linewidth=2)\n",
    "plt.title(\"Transformer Training Loss\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Translate \n",
    "model.eval()\n",
    "def greedy_translate(model, src_sentence, src_stoi, tgt_itos, max_len=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        src_ids = torch.tensor(encode_sentence(src_sentence, src_stoi, max_len), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        tgt_ids = torch.tensor([tgt_stoi[BOS_TOKEN]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        for i in range(max_len - 1):\n",
    "            logits = model(src_ids, tgt_ids)\n",
    "            next_token = torch.argmax(logits[0, -1, :], dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            tgt_ids = torch.cat([tgt_ids, next_token], dim=1)\n",
    "            if next_token.item() == tgt_stoi[EOS_TOKEN]:\n",
    "                break\n",
    "        ids = tgt_ids.squeeze(0).tolist()\n",
    "        words = []\n",
    "        for idx in ids:\n",
    "            tok = tgt_itos[idx]\n",
    "            if tok in (BOS_TOKEN, PAD_TOKEN): continue\n",
    "            if tok == EOS_TOKEN: break\n",
    "            words.append(tok)\n",
    "        return \" \".join(words)\n",
    "\n",
    "print(\"\\n--- Translations (greedy decode) ---\")\n",
    "test_sentences = [\"hello\", \"what is your name\", \"i love you\", \"good night\", \"see you later\"]\n",
    "for s in test_sentences:\n",
    "    out = greedy_translate(model, s, src_stoi, tgt_itos)\n",
    "    print(f\"EN: {s}  -->  VN: {out}\")\n",
    "\n",
    "# save model\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"src_itos\": src_itos,\n",
    "    \"tgt_itos\": tgt_itos,\n",
    "    \"src_stoi\": src_stoi,\n",
    "    \"tgt_stoi\": tgt_stoi\n",
    "}, \"tiny_transformer_en_vi.pth\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
